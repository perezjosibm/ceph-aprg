#!/usr/bin/env python3
"""
This module will parse a set of output that has been generated by top in batch mode.
It expects to be fed the output corresponding to one iteration of top.
"""

import argparse
import logging
import os
import sys
import json
import re
import tempfile
import pprint

# from json import JSONEncoder
import pandas as pd
import seaborn as sns

from top_entry import TopEntry, TopEntryJSONEncoder
from gnuplot_plate import GnuplotTemplate

__author__ = "Dave Pinkney and Jose J Palacios-Perez"

logger = logging.getLogger(__name__)
FORMAT = "[%(filename)s:%(lineno)s - %(funcName)20s() ] %(message)s"
pp = pprint.PrettyPrinter(width=61, compact=True)

DEFAULT_NUM_SAMPLES = 30
DEFAULT_CPU_RANGES = range(0, 111)  # Assume 112 cores by default

# The following info would be captured from the test_plan.json, but for now defined as default
PG_CPU_CORES_INFO = {
    # "num_cores": 4,
    # "cpu_mhz": 2400,
    # "cpu_model": "Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz",
    "OSD": "0-27,56-83",
    "FIO": "28-55,84-111",
}

# Options for Gnuplot might been better placed in the module that uses TopParser
# BTW, we pass them in the constructor


class TopParser(object):
    def __init__(
        self,
        fileName: str,
        jsonName: str = "",
        cpus: str = "",
        procs: str = "",
        num_samples: int = DEFAULT_NUM_SAMPLES,
        plotter_ops: dict = {},
    ):
        """
        Constructor
        We might need to extend the idea of using a fixed DEFAULT_NUM_SAMPLES to indicate
        the size of a data point to be combined with the FIO data
        However, its best to have a command line option to set this for flexibility
        """
        self.fileName = fileName  # Input _top.out file
        self.entries: list[TopEntry] = []
        self.jsonName = jsonName  # Output .json file _cores.json
        self.cpu_cores: list[dict] = []
        self.procs: list[dict] = []
        self.proc_groups: dict = {}  # This is the working dict
        self.cpus = cpus  # Input range of CPU cores to filter
        self.cpu_ranges = {}  # indexed by process group
        self.procs_file = procs  # .json file indexed with process group names and PIDs
        self.num_samples = 0
        # possible metrics to track per thread - cpu and mem are percentages, res and shr are in KB.
        # Only cpu is per thread, the other are per process
        self.metrics = ["cpu", "mem", "res", "shr"]
        self.core_cpu_metrics = ["user", "sys", "idle", "wait"]
        self.avg_cpus: dict = {}
        self.num_samples_per_run = num_samples
        # Ordered list by metric utilisation across process groups
        self.pgs_sorted = {}
        self.plotter_ops = plotter_ops

    def get_cpu_range(self):
        """
        Parse the string defining the range of CPU of interest -- we might
        provide this info in the input pids.json instead.We support a
        valid taskset string, eg. 0-3,5,7-9
        """

        def _update_cpu_ranges(cpu_str: str, pg: str):
            if pg not in self.cpu_ranges:
                self.cpu_ranges[pg] = []
            regex = re.compile(r"(\d+)([-](\d+))?")
            cpu_list_str = cpu_str.split(",")
            for cpu in cpu_list_str:
                matched = regex.search(cpu)
                if matched:
                    if matched.group(3):
                        self.cpu_ranges[pg].append(
                            range(int(matched.group(1)), int(matched.group(3)))
                        )
                    else:
                        self.cpu_ranges[pg].append([int(matched.group(1))])

        for pg in PG_CPU_CORES_INFO.keys():
            if self.cpus:
                _update_cpu_ranges(self.cpus, pg)
            else:
                # Use default ranges from PG_CPU_CORES_INFO
                _update_cpu_ranges(PG_CPU_CORES_INFO[pg], pg)

    def get_procs_names(self):
        """
        Get the processes names and pids from the json input file
        This json will be extended to contain the CPU cores ranges per process group
        """
        if self.procs_file:
            with open(self.procs_file, "r") as f:
                d = json.load(f)
                f.close()
            # the keys of d are the process name group (eg. OSD), whose values are
            # a list of PIDs
            for pg in d.keys():
                pids = map(
                    str, d[pg]
                )  # Input PIDs are integers, but job.getPid() returns strip
                # = str(x) for x in d[pg]
                self.proc_groups[pg] = {
                    "pids": list(pids),
                    "cpu_core_ranges": self.cpu_ranges[pg]
                    if pg in self.cpu_ranges
                    else DEFAULT_CPU_RANGES,
                    "threads": {},
                    "thr_pids": [],  # Add new pids as we find them
                    # Memory measurements are per process, so we need to track them separately
                    # This is RSS in KB
                    "res": {
                        "_data": [0.0] * self.num_samples,
                        "avg": 0.0,  # job.getCpu(),
                        "min": 0.0,
                        "max": 0.0,
                    },
                    # This is % of MEM in the system
                    "mem": {
                        "_data": [0.0] * self.num_samples,
                        "avg": 0.0,  # job.getMem(),
                        "min": 0.0,
                        "max": 0.0,
                    },
                    # This is SHR in KB
                    "shr": {
                        "_data": [0.0] * self.num_samples,
                        "avg": 0.0,  # job.getCpu(),
                        "min": 0.0,
                        "max": 0.0,
                    },
                    "num_samples": 0,
                    "sorted": {},
                    "avg_per_core": {},
                    "avg_per_run": {},
                }
            logger.debug(
                f"Parsing procs_file {self.procs_file} proc_groups: {self.proc_groups}"
            )

    def init_data_job(self, pg: str, job, num_samples: int):
        """
        Initalize a fixed lenght list for the _data field
        """
        self.proc_groups[pg]["threads"][job.getCommand()]["cpu"]["_data"] = [
            None
        ] * num_samples

    def aggregate_job(self, pg: str, job, index: int, is_parent: bool = False):
        """
        Aggregate this thread to the list of threads of interest
        If the is_parent is True, then this is the parent process for this group
        """
        logger.debug(f"Aggregating: {job} into {pg} at {index}")
        if job.getCommand() not in self.proc_groups[pg]["threads"]:
            self.proc_groups[pg]["threads"][job.getCommand()] = {
                "cpu": {
                    "_data": [0.0] * self.num_samples,
                    "avg": job.getCpu(),
                    "min": 0.0,
                    "max": 0.0,
                },
            }
        if is_parent:
            # For the parent process, we capture mem and res only once
            self.proc_groups[pg]["mem"]["_data"][index] = job.getMem()
            # REMEMBER: we now get default values as MB
            self.proc_groups[pg]["res"]["_data"][index] = job.getRes()
            self.proc_groups[pg]["shr"]["_data"][index] = job.getShr()
        else:
            if job.getPid() not in self.proc_groups[pg]["thr_pids"]:
                self.proc_groups[pg]["thr_pids"].append(job.getPid())
        # update entry with cummulative metric values
        self.proc_groups[pg]["threads"][job.getCommand()]["cpu"]["_data"][index] += (
            job.getCpu()
        )

    def get_job_stats(self, pg: str, metric: str):
        """
        Calculate the min, max and median of the cpu% metric  for each thread in the process group
        The final figure we might need to divide by 100
        """
        for comm, job in self.proc_groups[pg]["threads"].items():
            if metric in job:
                nentries = len(job[metric]["_data"])
                sum_metric = sum(job[metric]["_data"])
                if nentries > 0:
                    avg_metric = sum_metric / nentries
                else:
                    avg_metric = 0
                self.proc_groups[pg]["threads"][comm][metric]["avg"] = avg_metric
                self.proc_groups[pg]["threads"][comm][metric]["min"] = min(
                    job[metric]["_data"]
                )
                self.proc_groups[pg]["threads"][comm][metric]["max"] = max(
                    job[metric]["_data"]
                )

    def get_pg_stats(self, pg: str, metric: str):
        """
        Calculate the min, max and median of the metric (mem, res, shr) for each process group
        """
        pg_metric = self.proc_groups[pg][metric]
        pg_data = pg_metric["_data"]
        nentries = len(pg_data)
        sum_metric = sum(pg_data)
        if nentries > 0:
            avg_metric = sum_metric / nentries
        else:
            avg_metric = 0
        pg_metric["avg"] = avg_metric
        pg_metric["min"] = min(pg_data)
        pg_metric["max"] = max(pg_data)
        # self.proc_groups[pg][metric] = pg_metric

    def sort_pgs(self, metric: str):
        """
        Sort the list of process groups by metric utilisation
        """
        d = {}
        for pg_name, pg in self.proc_groups.items():
            d[pg_name] = pg[metric]["avg"]
        # dsorted = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}
        # self.proc_groups["sorted"][metric] = sorted(d, key=d.get, reverse=True)
        self.pgs_sorted[metric] = sorted(d, key=d.get, reverse=True)
        logger.debug(
            f"Sorted pgs by metric:{metric} : {pp.pformat(self.pgs_sorted[metric])}"
        )

    def sort_jobs(self, pg: str, metric: str):
        """
        Sort the list of threads by metric utilisation
        """
        d = {}
        for comm, job in self.proc_groups[pg]["threads"].items():
            d[comm] = job[metric]["avg"]
        # dsorted = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}
        self.proc_groups[pg]["sorted"][metric] = sorted(d, key=d.get, reverse=True)
        logger.debug(
            f"Sorted jobs for pg:{pg} metric:{metric} : {self.proc_groups[pg]['sorted'][metric]}"
        )

    def get_top_procs_util(self):
        """
        Sort the list of threads from top (metric) utilisation
        """
        for pg in self.proc_groups:
            for metric in self.metrics:
                if metric in ["mem", "res", "shr"]:
                    self.get_pg_stats(pg, metric)
                else:
                    self.get_job_stats(pg, metric)
                    self.sort_jobs(pg, metric)

        for metric in self.metrics:
            if metric in ["mem", "res", "shr"]:
                self.sort_pgs(metric)

    def get_procs_groups(self):
        """
        For each process group (keys of proc_groups), get the top 10
        threads utilisation, produce a sorted list, per metric
        - A new thread, its ppid is in the initial list of pids for the group
        - An existing thread, its pid is not in the initial list of pids for the group, but its ppid is
        present in the initial list of pids for the group
        - The parent process for this group is also included, as its pid is in the initial list of pids for the group
        """
        for i, entry in enumerate(self.entries):
            # logger.debug(f"Got: {pg} - {i} - {entry}")
            for (
                _,
                job,
            ) in entry.jobs.items():  # dict keys jobids=PID, value is a job object
                for pg, pgroup in self.proc_groups.items():
                    # logger.debug(f"Got: {jid}:{job.getPid()},{job.getPPid()} - {pgroup['pids']}")
                    if job.getPid() in pgroup["pids"]:
                        # This is the parent process for this group
                        self.aggregate_job(pg, job, i, True)
                        continue
                    a = set([job.getPid(), job.getPPid()])
                    b = set(pgroup["thr_pids"])
                    intersect = list(a & b)
                    maybe_command = job.getCommand() in pgroup["threads"].keys()
                    # logger.debug(f"Intersect: {intersect} of job{_jid}:{a} and pg{pg}:{b}")
                    if intersect or maybe_command:
                        # aggregate this thread to the list of threads (command and metrics)
                        self.aggregate_job(pg, job, i)

    def get_top_core_cpu_util(self):
        """
        For each CPU core, get the top ten most utilised (sys and user)
        """
        pass

    def get_core_util_per_run(self):
        """
        For each test run, get the combined CPU core utilisation, so can be
        embed with FIO .json data from a response curves run.
        """

        # for m in self.core_cpu_metrics:
        #     self.avg_cpus[m] = []
        #     for i in range(0, self.num_samples // DEFAULT_NUM_SAMPLES):
        #         self.avg_cpus[m].append(0.0)
        #     for coreid in self.avg_cpus['avg_per_core'].keys():
        #         for i in range(0, self.num_samples // DEFAULT_NUM_SAMPLES):
        #             self.avg_cpus[m][i] += self.avg_cpus['avg_per_core'][coreid][m][i]
        #         self.avg_cpus[m][i] /= len(self.avg_cpus['avg_per_core'].keys())
        # Init auxiliary dicts
        # avg_per_run = {}
        # for m in self.core_cpu_metrics:
        #     avg_per_run[m] = []
        def _get_core_util_per_run_per_pg(pg: str):
            avg_per_run = {m: [] for m in self.core_cpu_metrics}
            aux = {m: 0.0 for m in self.core_cpu_metrics}
            num_runs = self.num_samples // self.num_samples_per_run
            if num_runs == 0:
                num_runs = 1
            # avg_cpus = self.avg_cpus["avg_per_core"]
            avg_cpus = self.proc_groups[pg]["avg_per_core"]
            logger.info(
                f"pg:{pg}, num_runs: {num_runs}, num_samples_per_run: {self.num_samples_per_run}, avg_cpus keys: {avg_cpus.keys()}"
            )
            # Produce avg per test run, that is, combined for all cores running pg at each test run
            for m in self.core_cpu_metrics:
                for i in range(0, num_runs):
                    for coreid in avg_cpus.keys():
                        aux[m] += avg_cpus[coreid][m][i]
                    avg_per_run[m].append(aux[m] / len(avg_cpus.keys()))
                    aux[m] = 0.0
                # [ avg_per_run[m].append(sum(avg_cpus[coreid][m]) / len(avg_cpus[coreid][m])) for coreid in avg_cpus.keys() ]
            # self.avg_cpus["avg_per_run"] = avg_per_run
            self.proc_groups[pg]["avg_per_run"] = avg_per_run
            logger.info(f"{pg}: avg_per_run: {pp.pformat(avg_per_run)}")

        for pg in self.proc_groups:
            _get_core_util_per_run_per_pg(pg)

    def is_core_of_interest(self, coreid: int, pg: str) -> bool:
        """
        Check if the coreid is within the range of interest
        Alt. self.proc_groups[pg]['cpu_core_ranges']
        """
        for r in self.cpu_ranges[pg]:
            if coreid in r:
                return True
        return False

    def get_core_cpu_util(self):
        """
        self.cpu_cores traverse the list of CPU cores, each core is a dict with
        metrics, calculate the geometric average Might filter the cores of
        interest: either by the provided range or by the _threads.json file

        def _update_avg_per_cpu(coreid: str, metric: str, value: float):
            if coreid not in self.avg_cpus:
                self.avg_cpus[coreid] = {m: 0.0 for m in self.core_cpu_metrics}
            self.avg_cpus[coreid][metric] += value / self.num_samples
        """

        def _update_avg_per_cpu(
            _coreid: str, avg_cpus: dict, core: dict, _i: int, acum_per_cpu: dict
        ):
            if _coreid not in avg_cpus:
                avg_cpus[_coreid] = {m: [] for m in self.core_cpu_metrics}
            if _coreid not in acum_per_cpu:
                acum_per_cpu[_coreid] = {m: 0.0 for m in self.core_cpu_metrics}
            # Aggregate the core utilisation: we could use a translation dict
            acum_per_cpu[_coreid]["user"] += core["cpuUser"]
            acum_per_cpu[_coreid]["sys"] += core["cpuSystem"]
            acum_per_cpu[_coreid]["idle"] += core["cpuIdle"]
            acum_per_cpu[_coreid]["wait"] += core["cpuIoWait"]
            # Might extend to generic time stamps intervals: update the avg at the end of each run
            if ((_i + 1) % self.num_samples_per_run) == 0:
                if _i > 0:
                    for m in self.core_cpu_metrics:
                        avg_cpus[_coreid][m].append(
                            acum_per_cpu[_coreid][m] / self.num_samples_per_run
                        )
                        acum_per_cpu[_coreid] = {m: 0.0 for m in self.core_cpu_metrics}

        acum_per_cpu = {}
        for _i, entry in enumerate(self.entries):
            for _coreid, core in entry.cores.items():
                for pg in self.proc_groups:
                    logger.debug(f"Iter:{_i}: got cpu:{_coreid} - {core}")
                    avg_cpus = self.proc_groups[pg].get("avg_per_core", {})
                    if pg not in acum_per_cpu:
                        acum_per_cpu[pg] = {}
                    # Filter the cores of interest
                    if self.is_core_of_interest(int(_coreid), pg):
                        logger.debug(f"core: {_coreid} of interest for pg: {pg}")
                        # Aggregate the core utilisation -- we might do list comprehension
                        _update_avg_per_cpu(
                            _coreid, avg_cpus, core, _i, acum_per_cpu[pg]
                        )

        # logger.info(f"accum_per_cpu:{acum_per_cpu}")
        # self.avg_cpus["avg_per_core"] = avg_cpus # To be deprecated
        # self.proc_groups[pg]["avg_per_core"] = avg_cpus
        # Each list should be of the same size: num_samples // self.num_samples_per_run
        self.avg_cpus_size = self.num_samples // self.num_samples_per_run
        for m in self.core_cpu_metrics:
            for pg in self.proc_groups:
                avg_cpus = self.proc_groups[pg].get("avg_per_core", {})
                for coreid in avg_cpus.keys():
                    if len(avg_cpus[coreid][m]) != self.avg_cpus_size:
                        logger.error(
                            f"Error: {m} - {coreid} - {len(avg_cpus[coreid][m])} - {self.avg_cpus_size}"
                        )
        for pg in self.proc_groups:
            logger.info(
                f"avg_per_core: {pp.pformat(self.proc_groups[pg]['avg_per_core'])}"
            )

    def _gen_core_plot(self):
        """
        Generate a seaborn relplot chart for the avg_per_core and avg_per_run data
        Use name with suffixes with 'core_' and metrics
        Might assume only OSD is being used
        """
        name = self.fileName
        name += "_core.png"
        directory = os.path.dirname(name)
        basename = os.path.basename(name)
        # sns.set_theme(style="whitegrid", rc={'figure.figsize':(650,280)})
        # This would generate a plot per coreid, and that's not what we need
        # instead we want *all* core id plotted together, with one line per coreid

        sns.set_theme(style="darkgrid")
        for coreid in self.avg_cpus.keys():
            for pg in self.proc_groups:
                # data = self.avg_cpus[coreid]
                data = self.proc_groups[pg]["avg_per_core"][coreid]
                to_draw = pd.DataFrame(data)
                to_draw.columns.name = "Metrics"
                # sns.lineplot(x="timepoint", y="%CPU utilisation",
                #  hue="region", style="event",
                #  data=to_draw)
                # This is broken, need to understand the data structure
                g = sns.relplot(
                    data=to_draw,
                    kind="line",
                    markers=True,
                    # x="", y="%CPU", col="Classic", # the x and y are the columns of the dataframe , hue="event",
                ).set(title=name, ylabel="")
                g.figure.set_size_inches(15, 6)
                g.figure.set_dpi(100)
                _name = os.path.join(directory, f"core_{coreid}_{basename}")
                g.savefig(_name)

    def parse(self):
        """
        Load and parse the raw topOutput
        """
        logger.debug("Parsing file {0}".format(self.fileName))
        hasDate = None

        # Parse the file
        # Pass output sequence from top to TopParser
        with open(self.fileName, "r") as f:
            while True:
                firstLine = f.readline()
                if not firstLine:
                    break
                firstLine = firstLine.strip()
                if not firstLine:
                    # Skip blank lines between entries (if any)
                    continue
                # logger.debug('read line: "{0}"'.format(firstLine))
                topEntry = TopEntry(hasDate).parse(firstLine, f)
                self.entries.append(topEntry)
                hasDate = topEntry.hasDate
        self.num_samples = len(self.entries)

        logger.info(f"Parsed {self.num_samples} entries from {self.fileName}")
        # Organise per process and threads, and CPU cores

    def save_json(self, jsonName, data):
        """
        Save the parsed data to a JSON file
        """
        # Generate the JSON file: we might provide the key of the dict as argument
        if jsonName:
            with open(jsonName, "w", encoding="utf-8") as f:
                json.dump(
                    data,  # {'avg_cores': self.avg_cpus, 'proc_groups': self.proc_groups}
                    f,
                    sort_keys=True,
                    indent=4,
                    cls=TopEntryJSONEncoder,
                )
            logger.info(f"Saved JSON data to {self.jsonName}")

    def gen_plot(self):
        """
        Generate the gnuplot files
        """
        # Generate the gnuplot files for the process groups
        plot = GnuplotTemplate(
            self.fileName,
            self.proc_groups,
            self.num_samples,
            self.pgs_sorted,
            self.plotter_ops,
        )
        for metric in self.metrics:
            for pg in self.proc_groups:
                plot.genPlot(metric, pg)

        # Generate the gnuplot files for the CPU cores
        for pg in self.proc_groups:
            plot.genCorePlot(
                # self.avg_cpus["avg_per_run"],
                self.proc_groups[pg]["avg_per_run"],
                pg,
                f"{pg} CPU core utilisation",
                self.avg_cpus_size,
            )
        # Need to transverse the dict of avg_per_core
        # for coreid in self.avg_cpus['avg_per_core'].keys(): # one row per CPU core -- prob need one chart per metric
        #     apc_dict = { m:  self.avg_cpus['avg_per_core'][coreid][m] for m in self.core_cpu_metrics }
        # plot.genCorePlot(self.avg_cpus['avg_per_core'][coreid], f"core_{coreid}", "CPU core utilisation", self.avg_cpus_size)

    def run(self):
        """
        Main entry point
        """
        self.parse()
        self.get_procs_names()
        self.get_procs_groups()
        self.get_top_procs_util()
        self.get_cpu_range()
        self.get_core_cpu_util()
        self.get_core_util_per_run()
        logger.info(f"proc_groups is {pp.pformat(self.proc_groups)}")
        self.save_json(self.jsonName, self.avg_cpus)
        # self.save_json( self.jsonName, {'avg_cores': self.avg_cpus, 'proc_groups': self.proc_groups} )
        # self.save_json(self.jsonName, self.avg_cpus["avg_per_core"])
        self.gen_plot()
        # self._gen_core_plot()


def main(argv):
    examples = """
    Examples:
    # Parse top data from the specified output file, generated via "top -b":
        %prog topOutput.log

    # Parse top data from an output file containing timestamps, generated with a script run via cron such as:
    #
    #  date "+%m/%d %H:%M:%S" >> topWithDate.log
    #  top -b -n1 -H -1 >> topWithDate.log
        %prog topWithDate.log

    """
    parser = argparse.ArgumentParser(
        description="""This tool is used to parse output from the top command""",
        epilog=examples,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument("fileName", type=str, default=None, help="File to parse")
    parser.add_argument(
        "jsonName",
        type=str,
        default=None,
        nargs="?",
        help="JSON file to produce as output",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="True to enable verbose logging mode",
        default=False,
    )
    parser.add_argument(
        "-c",
        "--cpu",
        type=str,
        required=False,
        help="Range of CPUs to filter",
        default="",
    )
    parser.add_argument(
        "-p",
        "--pids",
        type=str,
        required=False,
        help="File with the process names and PIDs to filter",
        default="",
    )
    parser.add_argument(
        "-d", "--directory", type=str, help="Directory to examine", default="./"
    )
    parser.add_argument(
        "-n",
        "--num_samples",
        type=int,
        help="Number of samples that make a test run",
        default=DEFAULT_NUM_SAMPLES,
    )
    parser.add_argument(
        "-t",
        "--terminal",
        type=str,
        required=False,
        help="Term type for gnuplot: svg or png",
        default="png",
    )
    # Improvement: a profile to indicate how to process the gnuplot charts
    options = parser.parse_args(argv)

    if options.verbose:
        logLevel = logging.DEBUG
    else:
        logLevel = logging.INFO

    with tempfile.NamedTemporaryFile(dir="/tmp", delete=False) as tmpfile:
        # print(f"logname: {tmpfile.name}")
        logging.basicConfig(
            filename=tmpfile.name, encoding="utf-8", level=logLevel, format=FORMAT
        )

    logger.debug("Got options: {0}".format(options))

    os.chdir(options.directory)
    topParser = TopParser(
        options.fileName,
        options.jsonName,
        options.cpu,
        options.pids,
        options.num_samples,
        {"terminal": options.terminal},
    )
    topParser.run()


if __name__ == "__main__":
    main(sys.argv[1:])
